{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-kllYkw3Rzb",
        "outputId": "add32985-c641-403a-84dd-28083902c226"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading thinkplot.py\n",
            "Downloading thinkstats2.py\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "files_needed = [\n",
        "    {\"thinkplot.py\": \"https://github.com/AkeemSemper/ml_data/raw/main/thinkplot.py\"},\n",
        "    {\"thinkstats2.py\": \"https://github.com/AkeemSemper/ml_data/raw/main/thinkstats2.py\"},\n",
        "]\n",
        "current_folder = os.getcwd()\n",
        "for f in files_needed:\n",
        "    for file_name, url in f.items():\n",
        "        if not os.path.exists(file_name):\n",
        "            print(f\"Downloading {file_name}\")\n",
        "            os.system(f\"curl {url} -o {current_folder}/{file_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Io94-za_3Rze"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import thinkstats2\n",
        "import thinkplot\n",
        "from scipy import stats as ss\n",
        "\n",
        "##Seaborn for fancy plots.\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.rcParams[\"figure.figsize\"] = (8,8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IygWbjYy3Rzf"
      },
      "source": [
        "<h1>Quiz 3</h1>\n",
        "\n",
        "Please fill in the bodies of the functions as specified. Please read the instructions closely and ask for clarification if needed. A few notes/tips:\n",
        "<ul>\n",
        "<li>Like all the functions we use, the function is a self contained thing. It takes in values as paramaters when called, and produces a return value. All of the inputs that may change should be in that function call, imagine your function being cut/pasted into some other file - it should not depend on anything outside of libraries that it may need.\n",
        "<li>Test your function with more than one function call, with different inputs. See an example in comments below the first question.\n",
        "<li>If something doesn't work, print or look at the varaibles window. The #1 skill that'll allow you to write usable code is the ability to find and fix errors. Printing a value out line by line so you can see how it changes, and looking for the step where something goes wrong is A-OK and pretty normal. It is boring.\n",
        "<li>Unless otherwise specified, you can use outside library functions to calculate things.\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvL3lG2A3Rzg"
      },
      "source": [
        "<h1>Test Data</h1>\n",
        "\n",
        "You may notice there's no data specified or attached. You'll need to generate some test data if you want to test your functions.\n",
        "\n",
        "The easiest way to generate test data is to use some of the random functions to generate data that looks like what you need. Numpy random and scipy disributions .rvs functions are good places to look, we've also generated random data many times in the past.\n",
        "\n",
        "There is no specific requirement on what your data needs to be, it just needs to be good enough to test your function. If you pay attention to what exactly you're calculating and the criteria given, you should be able to create some suitable data for different tests. As an example, for the Hyp Test question, you need two sets of normal data. You can generate some in many ways, one is through scipy:\n",
        "<ul>\n",
        "<li>ss.norm.rvs(loc=0, scale=1, size=1, random_state=None)\n",
        "</ul>\n",
        "<p>\n",
        "Since you're checking if there's a significant difference between the two groups, you'd likely want multiple sets of data - two that are very close, so they will not show a difference, and two that are not close, so they will show a difference. Think about what you are checking, then just make some data that will allow you to test that.\n",
        "\n",
        "This should not be extremely difficult to code nor should it be super time consuming, the commands are pretty simple and generating random varaibles is pretty similar for any distribution. There is some though involved in saying \"what data do I need to check this?\" That's something that is pretty important in general, if we are creating something we need to make sure that it works in general, not just one example. Critically, there are not specific sets of data you need - almost anything will work. It is only there to let your functions run and see if they are correct. You don't need to aim for \"the perfect test data\" or anything like that, just make some data in a list, if it needs to be of a certain distribution, use that dist to get it; if the distribution doesn't matter, just make something."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsU_nUQx3Rzg"
      },
      "source": [
        "<h1>Ski on Chi - 10pts</h1>\n",
        "\n",
        "You operate a ski hill, and over the years you've seen the distribution of skiers vs snowboarders vs snow skaters etc... change a bit. This is your first full open season since the pandemic hit. When you closed in early 2020, the distribution of your customer base was:\n",
        "<ul>\n",
        "<li>Skiers - 40%\n",
        "<li>Snowboarders - 20%\n",
        "<li>Snow Skaters - 5%\n",
        "<li>Non-Active (i.e. sit in the lodger) - 15%\n",
        "<li>Lesson takers - 20%\n",
        "</ul>\n",
        "\n",
        "You are seeing a different pattern now, but you are not sure if that is due to a change in what your customers want or due to just random chance. You want to be able to analytically tell if what you observe each week is a real change from that baseline above, or nothing to worry about.\n",
        "\n",
        "In this function you'll take in:\n",
        "<ul>\n",
        "<li>Two list of values for the observed number of customers in each group, in the order indicated above. E.g. [35,25,10,10,20].\n",
        "<li>An alpha value (the cutoff criteria for a p-values)\n",
        "</ul>\n",
        "<br><br>\n",
        "You'll return 3 results:\n",
        "<ul>\n",
        "<li>A true/false assessment for if the data appears to show a significant difference in means, measured by if the pValue is less than the supplied alpha.\n",
        "<li>The name of the category that MOST EXCEEDS the expectation.\n",
        "<li>The name of the cetegory that is MOST EXCEEDED BY the expectation.\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8LMcgz63Rzh",
        "outputId": "cb746125-08c3-4920-fdae-2d9a35514f7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False Snowboarders Skiers\n"
          ]
        }
      ],
      "source": [
        "def skiCustomersChange(observedCustys, alpha=.05):\n",
        "\n",
        "    # We take the data from the problem question\n",
        "    baseline_pct = np.array([0.40, 0.20, 0.05, 0.15, 0.20])\n",
        "    categories = np.array(\n",
        "        [\"Skiers\", \"Snowboarders\", \"Snow Skaters\", \"Non-Active\", \"Lesson takers\"])\n",
        "\n",
        "    observed = np.array(observedCustys)\n",
        "    total_customers = observed.sum()\n",
        "\n",
        "    # This is the expected counts\n",
        "    expected = baseline_pct * total_customers\n",
        "\n",
        "    # Chi-square goodness-of-fit test\n",
        "    chi2_stat, p_value = ss.chisquare(f_obs=observed, f_exp=expected)\n",
        "\n",
        "    isSignificantDiff = p_value < alpha\n",
        "\n",
        "    # Differences from expectation for each category\n",
        "    diff_counts = observed - expected\n",
        "\n",
        "    # Category most above / most below expectation\n",
        "    higherThanExp = categories[diff_counts.argmax()]\n",
        "    lowerThanExp  = categories[diff_counts.argmin()]\n",
        "\n",
        "    return isSignificantDiff, higherThanExp, lowerThanExp\n",
        "\n",
        "diff, highCategory, lowCategory = skiCustomersChange([35,25,10,10,20], .05)\n",
        "print(diff, highCategory, lowCategory)\n",
        "\n",
        "# Once we run this we will be able to see T/F and which categories exceeds the\n",
        "# expectation and exceeded by the expectation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_vxuSa4J3Rzh"
      },
      "outputs": [],
      "source": [
        "#Example function calls\n",
        "# diff, highCategory, lowCategory = skiCustomersChange([35,25,10,10,20], .05)\n",
        "# diff, highCategory, lowCategory = skiCustomersChange([15,40,15,10,20], .1)\n",
        "# diff, highCategory, lowCategory = skiCustomersChange([40,10,10,10,30], .01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO8C7BIr3Rzh"
      },
      "source": [
        "<h2>Hypothesis Testing - 10pts</h2>\n",
        "\n",
        "In this function you'll take in:\n",
        "<ul>\n",
        "<li>Two list of values - dataA and dataB. The data will be normally distributed.\n",
        "<li>An alpha value (the cutoff criteria for a p-values)\n",
        "<li>A power value (the likelihood of not getting a false negative)\n",
        "<li>An effect size value.\n",
        "</ul>\n",
        "<br><br>\n",
        "You'll produce a tuple of 3 results:\n",
        "<ul>\n",
        "<li>A true/false assessment for if the data appears to show a significant difference in means, measured by if the pValue is less than the supplied alpha in a t-test.\n",
        "<li>A true/false assessment for if a hypothesis test has enough power to be reliable, measured by if the power you calculate is greater than the supplied power.\n",
        "<li>A true false assessment for if the data appears to show a significant difference in means, measured by if the Cohen effect size is greater than the supplied effect size.\n",
        "</ul>\n",
        "\n",
        "<b>Please report your responses in the format indicated in the template. As well, please report all true/false values as 1/0. 1 is True, 0 is false. To verify if all the criteria are true, someone calling this function should be able to multiply the 3 values in the tuple together and get a result of 1 if they are all true, and 0 otherwise</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "e0d1H-5B3Rzi",
        "outputId": "4ed95efc-2c24-44e7-a2dd-c2b018ea21b8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dataA' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3691862008.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mdataB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m19\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m21\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m22\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrengthOfEffect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meffectSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dataA' is not defined"
          ]
        }
      ],
      "source": [
        "def strengthOfEffect(dataA, dataB, alpha=.05, power=.8, effectSize=.5):\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    a = np.array(dataA)\n",
        "    b = np.array(dataB)\n",
        "\n",
        "    # This is the t-test for significant difference in means\n",
        "    t_stat, p_value = ss.ttest_ind(a, b, equal_var=False)\n",
        "    passedPtest = 1 if p_value < alpha else 0\n",
        "\n",
        "    # Now we made a simple power calculation for our code\n",
        "    pooled_std = np.sqrt(((a.var(ddof=1) + b.var(ddof=1)) / 2))\n",
        "    cohen_d = abs(a.mean() - b.mean()) / pooled_std\n",
        "\n",
        "    analysis = TTestIndPower()\n",
        "    calc_power = analysis.solve_power(effect_size=cohen_d, nobs1=len(a), alpha=alpha,\n",
        "    ratio=len(b) / len(a), alternative='two-sided')\n",
        "\n",
        "    passedPower = 1 if calc_power > power else 0\n",
        "\n",
        "    # Effect size comparison\n",
        "    passedEffectSize = 1 if cohen_d > effectSize else 0\n",
        "\n",
        "    results = (passedPtest, passedPower, passedEffectSize)\n",
        "    return results\n",
        "\n",
        "    dataA = [10, 12, 11, 13, 9]\n",
        "    dataB = [20, 19, 21, 18, 22]\n",
        "\n",
        "print(strengthOfEffect(dataA, dataB, alpha=.05, power=.8, effectSize=.5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNVSJzVX3Rzi"
      },
      "outputs": [],
      "source": [
        "#Example function calls\n",
        "# results = strengthOfEffect(oneListOfValues, anotherListOfValues, .05, .9, .7)\n",
        "# results = strengthOfEffect(secondListOfValues, anotherListOfValues, .03, .7, .4)\n",
        "# results = strengthOfEffect(oneListOfValues, moreListOfValues, .05, .8, .7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEBF-Nml3Rzi"
      },
      "source": [
        "<h2>Safe Test - 10pts</h2>\n",
        "\n",
        "In this function you'll take in:\n",
        "<ul>\n",
        "<li>Two list of values - dataA and dataB.\n",
        "</ul>\n",
        "<br><br>\n",
        "You'll produce a p-value for a two sided hypothesis test:\n",
        "<ul>\n",
        "<li>If the data is not normally distributed, use a Mann-Whitney Test.\n",
        "<li>If the data appears to be normally distributed, and the variance differs substantially, use a Welch's t-test.\n",
        "<li>If none of those conditions are true, use a 'normal' (Student's) t-test.\n",
        "<li>Note: The execution of all of these tests are very similar from your persepective. They are all in the scipy documentation - Google for exact details, and the code closely mirrors the examples we did.\n",
        "<li>Note 2: If you ever need to use a cutoff for a p-value in the middle of your calculations, please choose something reasonable. There are common defaults for whatever you may need. These defaults are likely shown in the documentation or any examples you may look up.\n",
        "</ul>\n",
        "\n",
        "<b>In any case, the value returned is one number (not in a list, tuple, etc...) that is the pValue performed for that test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bCVHwym3Rzi",
        "outputId": "78464d81-4d9d-4e82-b568-d151e9eb1411"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.34659350708733416\n"
          ]
        }
      ],
      "source": [
        "def flexHypTest(dataA, dataB):\n",
        "  a = np.array(dataA)\n",
        "  b = np.array(dataB)\n",
        "\n",
        "    # Check normality Shapiro-Wilk\n",
        "    statA, pA = ss.shapiro(a)\n",
        "    statB, pB = ss.shapiro(b)\n",
        "    alpha_norm = 0.05\n",
        "\n",
        "    if pA < alpha_norm or pB < alpha_norm:\n",
        "        # Not normal using Mann-Whitney U test\n",
        "        stat, pvalue = ss.mannwhitneyu(a, b, alternative='two-sided')\n",
        "    else:\n",
        "        # Normal check variance equality Levene's test\n",
        "        stat_lev, p_lev = ss.levene(a, b)\n",
        "        alpha_var = 0.05\n",
        "\n",
        "        if p_lev < alpha_var:\n",
        "            # Variances differ using Welch's t-test\n",
        "            stat, pvalue = ss.ttest_ind(a, b, equal_var=False)\n",
        "        else:\n",
        "            # Variances similar using Student's t-test\n",
        "            stat, pvalue = ss.ttest_ind(a, b, equal_var=True)\n",
        "\n",
        "    return pvalue\n",
        "\n",
        "# Lastly we want the output\n",
        "print(flexHypTest([1, 2, 3, 4, 5], [2, 3, 4, 5, 6]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lumt5sK3Rzj"
      },
      "source": [
        "<h1>Grade Distribution - 10pts</h1>\n",
        "\n",
        "Grade distributions for final letter grades at a school are generally skewed towards the higher end of the scale. We can model it with a function below.\n",
        "\n",
        "Percentage grades on individual assignments are often skewnormally distributed. (Note: this is more for curved schools than somewhere like NAIT with hard cutoffs. When I was in school CompSci profs would aim for a 50%-60% raw average to get a normal-ish distribution of marks.)\n",
        "\n",
        "You are seeking to generate a grading system, in two steps:\n",
        "<ul>\n",
        "<li>Use the supplied Weibull distribution in the simpleGenerateLetterGradeBuckets function to generate the distribution of letter grades - A,B,C,D,F. We are a simple school and we only have letters, no plus or minus.\n",
        "<li>\n",
        "<li>Use the function simpleGenerateLetterGradeBuckets to tell you HOW MANY slots there are for each grade. This is done for you in the provided function, you just need to call it and get the results. Please pay attention to the n value for number.\n",
        "<li>Take the supplied raw percentage grades and fit them into those buckets. I.E. if there are 17 slots for an A grade, the 17 highest percentage marks should get an A; if there are then 52 for B, then the next 52 highest get a B, etc...\n",
        "<li><b>You are going to return a list of tuples - the original percentage grade, and the letter grade. E.g. [(72,B), (84,A), etc...]</b>\n",
        "</ul>\n",
        "\n",
        "<br><br>\n",
        "In this function you'll take in:\n",
        "<ul>\n",
        "<li>A list of raw percentage grades, from 0 to 100. E.g. [100,98,24,53,45, etc...]\n",
        "</ul>\n",
        "\n",
        "You'll produce:\n",
        "<ul>\n",
        "<li>A list of tuples. Each tuple is the original percentage grade, and the letter grade. .\n",
        "</ul>\n",
        "\n",
        "<br>\n",
        "Note: You'll have to run the function cell down at the bottom first.\n",
        "<br><br>\n",
        "<b>Bonus: The provided function for grade buckets probably isn't the best overall, if you can rewrite it to be better, up to 3 bonus marks. Think about the random factor...</b>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def assignLetterGrades(rawPercentageGrades):\n",
        "    # First we get bucket sizes for A,B,C,D,F\n",
        "    gradeBuckets = simpleGenerateLetterGradeBuckets()\n",
        "\n",
        "    # 2. Sort grades from highest to lowest\n",
        "    sortedGrades = sorted(rawPercentageGrades, reverse=True)\n",
        "\n",
        "    # 3.Then Assign grades according to bucket sizes\n",
        "    letterAssignments = {}\n",
        "    index = 0\n",
        "\n",
        "    for letter in ['A', 'B', 'C', 'D', 'F']:\n",
        "        letterAssignments[letter] = sortedGrades[index : index + gradeBuckets[letter]]\n",
        "        index += gradeBuckets[letter]\n",
        "\n",
        "    # 4. Create the output tuples (original grade, letter grade)\n",
        "    result = []\n",
        "\n",
        "    for grade in rawPercentageGrades:   # keep original order\n",
        "        assignedLetter = None\n",
        "        for letter in letterAssignments:\n",
        "            if grade in letterAssignments[letter]:\n",
        "                assignedLetter = letter\n",
        "                letterAssignments[letter].remove(grade)  # avoid duplicates\n",
        "                break\n",
        "        result.append((grade, assignedLetter))\n",
        "\n",
        "    return result\n",
        "\n",
        "print(assignLetterGrades([100, 98, 92, 90]))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd3UU8lq___T",
        "outputId": "2e3c5d19-b2ca-4a31-ff98-41b38bf6e342"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(100, 'A'), (98, 'A'), (92, 'A'), (90, 'A')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeiy1x0G3Rzj",
        "outputId": "7d8ba4c4-6a2a-4ff2-e3b8-9e64e904d859"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'A': 65, 'B': 173, 'C': 121, 'D': 46, 'F': 18}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Example for 423 students\n",
        "simpleGenerateLetterGradeBuckets(423)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "0B2jCUJg3Rzj"
      },
      "outputs": [],
      "source": [
        "def simpleGenerateLetterGradeBuckets(n=100):\n",
        "    #Define distribution params\n",
        "    c = 1.5\n",
        "    loc = 3\n",
        "    scale = 1.5\n",
        "\n",
        "    # Define cuts for 'data' values (7.2 - X), then convert to X-space for CDF calculation\n",
        "    # A: 7.2 - X > 3.7  => X < 3.5\n",
        "    # B: 7.2 - X > 2.9  => X < 4.3\n",
        "    # C: 7.2 - X > 1.9  => X < 5.3\n",
        "    # D: 7.2 - X > 0.9  => X < 6.3\n",
        "    # F: 7.2 - X <= 0.9 => X >= 6.3\n",
        "    cut_points_x = [3.5, 4.3, 5.3, 6.3]\n",
        "\n",
        "    # Calculate CDF at these points to get theoretical probabilities for each grade\n",
        "    cdf_values = [ss.weibull_min.cdf(x, c, loc, scale) for x in cut_points_x]\n",
        "\n",
        "    # Calculate theoretical probabilities for each grade category\n",
        "    # P(X < 3.5)\n",
        "    prob_A = cdf_values[0]\n",
        "    # P(3.5 <= X < 4.3)\n",
        "    prob_B = cdf_values[1] - cdf_values[0]\n",
        "    # P(4.3 <= X < 5.3)\n",
        "    prob_C = cdf_values[2] - cdf_values[1]\n",
        "    # P(5.3 <= X < 6.3)\n",
        "    prob_D = cdf_values[3] - cdf_values[2]\n",
        "    # P(X >= 6.3)\n",
        "    prob_F = 1.0 - cdf_values[3]\n",
        "\n",
        "    probs = [prob_A, prob_B, prob_C, prob_D, prob_F]\n",
        "    grade_letters = [\"A\", \"B\", \"C\", \"D\", \"F\"]\n",
        "    buckets = {letter: 0 for letter in grade_letters}\n",
        "\n",
        "    remaining_n = n\n",
        "\n",
        "    # Calculate initial bucket sizes by rounding and ensure sum is n\n",
        "    for i, letter in enumerate(grade_letters):\n",
        "        count = int(round(probs[i] * n))\n",
        "        buckets[letter] = count\n",
        "        remaining_n -= count\n",
        "\n",
        "    # Distribute any remaining students due to rounding errors\n",
        "    # This ensures the sum of buckets is exactly n\n",
        "    if remaining_n != 0:\n",
        "        # Find the grade with the largest proportion to adjust\n",
        "        # This is a simple heuristic to distribute rounding error\n",
        "        max_prob_idx = np.argmax(probs)\n",
        "        buckets[grade_letters[max_prob_idx]] += remaining_n\n",
        "\n",
        "    return buckets"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rk3-_8So-1w0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "ea39297c2a3b8433e0e3c4b620aff79df88eb4bda961dfb2311fbafd7efdbd77"
    },
    "kernelspec": {
      "display_name": "Python 3.8.11 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}